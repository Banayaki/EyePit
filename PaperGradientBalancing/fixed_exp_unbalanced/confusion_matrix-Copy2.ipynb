{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/makiflow/core/inference/maki_core.py:108: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from makiflow.models.classificator import Classificator\n",
    "from makiflow.metrics import bin_categorical_dice_coeff\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "from makiflow import set_main_gpu\n",
    "set_main_gpu(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 8\n",
    "IMAGE_SHAPE = (1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(arch_path, weights_path):\n",
    "    model = Classificator.from_json(arch_path)\n",
    "    model.set_session(SESSION)\n",
    "    model.load_weights(weights_path)\n",
    "    return model\n",
    "\n",
    "def load_im_mask(image_path, masks_folder_path):\n",
    "    image_shape = IMAGE_SHAPE\n",
    "    n_classes = N_CLASSES\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    mask_folder = masks_folder_path\n",
    "\n",
    "    label_tensor = np.zeros(shape=(*image_shape, n_classes + 1), dtype='int32')\n",
    "    for binary_mask_path in glob(os.path.join(mask_folder, '*')):\n",
    "        filename = binary_mask_path.split('/')[-1]\n",
    "        class_id = int(filename.split('.')[0])\n",
    "        assert class_id != 0, 'Encountered class 0. Class names must start from 1.'\n",
    "        binary_mask = cv2.imread(binary_mask_path)\n",
    "        assert binary_mask is not None, f'Could not load mask with name={binary_mask_path}'\n",
    "        label_tensor[..., class_id] = binary_mask[..., 0] * class_id\n",
    "    label_tensor = np.max(label_tensor, axis=-1)\n",
    "    \n",
    "    return image.astype(np.float32, copy=False) / 255, label_tensor\n",
    "\n",
    "def load_data(config_path):\n",
    "    config = load_json(config_path)\n",
    "    config = config['test_config']\n",
    "    image_paths = config['test_image']\n",
    "    mask_paths = config['test_mask']\n",
    "    \n",
    "    images = []\n",
    "    masks = []\n",
    "    for image_path, mask_folder_path in zip(image_paths, mask_paths):\n",
    "        image, mask = load_im_mask(image_path, mask_folder_path)\n",
    "        images.append(image)\n",
    "        masks.append(mask)\n",
    "    return images, masks\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "def test(arch_path, weights_path, config_path, th=0.5):\n",
    "    images, masks = load_data(config_path)\n",
    "    model = create_model(arch_path, weights_path)\n",
    "    \n",
    "    predictions = []\n",
    "    max_preds = []\n",
    "    for image in tqdm(images):\n",
    "        pred = (model.predict([image])[0] > th).astype('uint8') * np.arange(1, 9, 1).reshape(1, 1, 8)\n",
    "#         pred = pred.max(axis=-1)\n",
    "        predictions += [pred]\n",
    "        max_preds += [pred.max(axis=-1)]\n",
    "    \n",
    "    # predictions = np.concatenate(predictions, axis=0)\n",
    "    f1_scores = f1_score(np.asarray(masks).reshape(-1), np.asarray(max_preds).reshape(-1), average=None)\n",
    "    return confusion_matrix(np.asarray(masks).reshape(-1), np.asarray(max_preds).reshape(-1)), f1_scores\n",
    "\n",
    "\n",
    "def compute_tpr_tnr_class(class_id, C):\n",
    "    TN = C[0, 0]\n",
    "    TP = C[class_id, class_id]\n",
    "    \n",
    "    FP = 0\n",
    "    for i in range(0, C.shape[0]):\n",
    "        if i == class_id:\n",
    "            continue\n",
    "        FP += C[i, class_id]\n",
    "        \n",
    "    FN = 0\n",
    "    for i in range(0, C.shape[0]):\n",
    "        if i == class_id:\n",
    "            continue\n",
    "        FN += C[class_id, i]\n",
    "    \n",
    "    tpr = 1 - FN / (FN + TP + 1e-7)\n",
    "    tnr = 1 - FP / (FP + TN + 1e-7)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "    return tpr, tnr, acc\n",
    "    \n",
    "def compute_tpr(C):\n",
    "    trps = []\n",
    "    tnrs = []\n",
    "    accs = []\n",
    "    for i in range(1, C.shape[0]):\n",
    "        rates = compute_tpr_tnr_class(i, C)\n",
    "        trps.append(rates[0])\n",
    "        tnrs.append(rates[1])\n",
    "        accs.append(rates[2])\n",
    "    return trps, tnrs, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/makiflow/layers/untrainable_layers.py:44: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/makiflow/layers/untrainable_layers.py:841: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
      "\n",
      "Model is restored!\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/makiflow/core/inference/maki_core.py:117: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/makiflow/core/inference/model_serializer.py:49: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from cv3_fold0/result_1/epoch_13/weights.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:05,  5.64s/it]\u001b[A\n",
      " 12%|█▎        | 1/8 [00:07<00:51,  7.41s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.43s/it]\u001b[A\n",
      " 25%|██▌       | 2/8 [00:13<00:42,  7.15s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.73s/it]\u001b[A\n",
      " 38%|███▊      | 3/8 [00:18<00:31,  6.32s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.96s/it]\u001b[A\n",
      " 50%|█████     | 4/8 [00:25<00:26,  6.62s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:04,  4.07s/it]\u001b[A\n",
      " 62%|██████▎   | 5/8 [00:32<00:20,  6.68s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.20s/it]\u001b[A\n",
      " 75%|███████▌  | 6/8 [00:39<00:13,  6.63s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.08s/it]\u001b[A\n",
      " 88%|████████▊ | 7/8 [00:45<00:06,  6.55s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.92s/it]\u001b[A\n",
      "100%|██████████| 8/8 [00:51<00:00,  6.43s/it]\n"
     ]
    }
   ],
   "source": [
    "cv3_f0_model_arch = 'cv3_fold0/result_1/model.json'\n",
    "cv3_f0_model_weights = 'cv3_fold0/result_1/epoch_13/weights.ckpt'\n",
    "cv3_f0_config = 'cv3_fold0/config.json'\n",
    "\n",
    "cv3_f0_C, f1_scores_1 = test(cv3_f0_model_arch, cv3_f0_model_weights, cv3_f0_config, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr0, tnr0, accs0 = np.array(compute_tpr(cv3_f0_C)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98, 0.86, 0.91, 0.81, 0.18, 0.25, 0.14, 0.87])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is restored!\n",
      "INFO:tensorflow:Restoring parameters from cv3_fold1/result/epoch_13/weights.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:05,  5.64s/it]\u001b[A\n",
      " 12%|█▎        | 1/8 [00:05<00:39,  5.71s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.29s/it]\u001b[A\n",
      " 25%|██▌       | 2/8 [00:13<00:38,  6.38s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.22s/it]\u001b[A\n",
      " 38%|███▊      | 3/8 [00:19<00:31,  6.35s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.11s/it]\u001b[A\n",
      " 50%|█████     | 4/8 [00:23<00:21,  5.44s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.21s/it]\u001b[A\n",
      " 62%|██████▎   | 5/8 [00:26<00:14,  4.85s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.33s/it]\u001b[A\n",
      " 75%|███████▌  | 6/8 [00:30<00:08,  4.47s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.07s/it]\u001b[A\n",
      " 88%|████████▊ | 7/8 [00:33<00:04,  4.11s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.18s/it]\u001b[A\n",
      "100%|██████████| 8/8 [00:41<00:00,  5.16s/it]\n"
     ]
    }
   ],
   "source": [
    "cv3_f1_model_arch = 'cv3_fold1/result/model.json'\n",
    "cv3_f1_model_weights = 'cv3_fold1/result/epoch_13/weights.ckpt'\n",
    "cv3_f1_config = 'cv3_fold1/config.json'\n",
    "\n",
    "cv3_f1_dice, f1_scores_2 = test(cv3_f1_model_arch, cv3_f1_model_weights, cv3_f1_config, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr1, tnr1, accs1 = np.array(compute_tpr(cv3_f1_dice)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98, 0.72, 0.92, 0.81, 0.79, 0.03, 0.48, 0.95])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is restored!\n",
      "INFO:tensorflow:Restoring parameters from cv3_fold2/result_1/epoch_13/weights.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:06,  6.82s/it]\u001b[A\n",
      " 12%|█▎        | 1/8 [00:07<00:49,  7.12s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.27s/it]\u001b[A\n",
      " 25%|██▌       | 2/8 [00:13<00:40,  6.79s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.94s/it]\u001b[A\n",
      " 38%|███▊      | 3/8 [00:18<00:31,  6.39s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.28s/it]\u001b[A\n",
      " 50%|█████     | 4/8 [00:23<00:23,  5.94s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.90s/it]\u001b[A\n",
      " 62%|██████▎   | 5/8 [00:28<00:17,  5.80s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:02,  2.96s/it]\u001b[A\n",
      " 75%|███████▌  | 6/8 [00:34<00:11,  5.68s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.09s/it]\u001b[A\n",
      " 88%|████████▊ | 7/8 [00:40<00:05,  5.67s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:03,  3.04s/it]\u001b[A\n",
      "100%|██████████| 8/8 [00:45<00:00,  5.73s/it]\n"
     ]
    }
   ],
   "source": [
    "cv3_f2_model_arch = 'cv3_fold2/result_1/model.json'\n",
    "cv3_f2_model_weights = 'cv3_fold2/result_1/epoch_13/weights.ckpt'\n",
    "cv3_f2_config = 'cv3_fold2/config.json'\n",
    "\n",
    "cv3_f2_dice, f1_scores_3 = test(cv3_f2_model_arch, cv3_f2_model_weights, cv3_f2_config, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr2, tnr2, accs2 = np.array(compute_tpr(cv3_f2_dice)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_1 = f1_scores_1[1:].round(2)\n",
    "f1_scores_1 = np.hstack([f1_scores_1, np.mean(f1_scores_1).round(2)])\n",
    "f1_scores_2 = f1_scores_2[1:].round(2)\n",
    "f1_scores_2 = np.hstack([f1_scores_2, np.mean(f1_scores_2).round(2)])\n",
    "f1_scores_3 = f1_scores_3[1:].round(2)\n",
    "f1_scores_3 = np.hstack([f1_scores_3, np.mean(f1_scores_3).round(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr0 = np.hstack([tpr0, np.mean(tpr0).round(2)])\n",
    "tpr1 = np.hstack([tpr1, np.mean(tpr1).round(2)])\n",
    "tpr2 = np.hstack([tpr2, np.mean(tpr2).round(2)])\n",
    "\n",
    "tnr0 = np.hstack([tnr0, np.mean(tnr0).round(2)])\n",
    "tnr1 = np.hstack([tnr1, np.mean(tnr1).round(2)])\n",
    "tnr2 = np.hstack([tnr2, np.mean(tnr2).round(2)])\n",
    "\n",
    "accs0 = np.hstack([accs0, np.mean(accs0).round(2)])\n",
    "accs1 = np.hstack([accs1, np.mean(accs1).round(2)])\n",
    "accs2 = np.hstack([accs2, np.mean(accs2).round(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.97, 0.73, 0.92, 0.83, 0.51, 0.1 , 0.32, 0.89, 0.66]),\n",
       " array([0.98, 0.99, 0.74, 0.99, 1.  , 1.  , 1.  , 0.95, 0.96]),\n",
       " array([0.98, 0.99, 0.76, 0.99, 1.  , 1.  , 0.99, 0.95, 0.96]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_tpr = np.mean([tpr0, tpr1, tpr2], axis=0).round(2)\n",
    "mean_tnr = np.mean([tnr0, tnr1, tnr2], axis=0).round(2)\n",
    "mean_acc = np.mean([accs0, accs1, accs2], axis=0).round(2)\n",
    "mean_tpr, mean_tnr, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.975, 0.86 , 0.83 , 0.91 , 0.755, 0.55 , 0.66 , 0.92 , 0.81 ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ba = (mean_tpr + mean_tnr) / 2\n",
    "ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78, 0.67, 0.48, 0.52, 0.47, 0.14, 0.29, 0.38, 0.46])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_f1 = np.mean([f1_scores_1, f1_scores_2, f1_scores_3], axis=0).round(2)\n",
    "mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=np.stack([tpr0, tpr1, tpr2, f1_scores_1, f1_scores_2, f1_scores_3, mean_tpr, mean_tnr, mean_f1, mean_acc, ba]), \n",
    "    index=['test0 tpr', 'test1 tpr', 'test2 tpr', 'test0 f1',\n",
    "           'test1 f1', 'test2 f1', 'mean_tpr', 'mean_tnr', 'mean_f1', 'mean_acc', 'ba'],\n",
    "    columns=list(range(1,9)) + ['mean']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tpr_unbalanced_p0.5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
