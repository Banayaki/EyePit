{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from makiflow.layers import *\n",
    "#from makizoo.backbones.vgg import VGG16\n",
    "import makizoo as mz\n",
    "from makiflow.base import MakiModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import makiflow as mf\n",
    "mf.set_main_gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from makiflow.base.base_layers import BatchNormBaseLayer\n",
    "class BatchNormLayer(BatchNormBaseLayer):\n",
    "    def __init__(self, D, name, decay=0.9, eps=1e-4, use_gamma=True,\n",
    "                 use_beta=True, mean=None, var=None, gamma=None, beta=None):\n",
    "        \"\"\"\n",
    "        Batch Normalization Procedure:\n",
    "            X_normed = (X - mean) / variance\n",
    "            X_final = X*gamma + beta\n",
    "        gamma and beta are defined by the NN, e.g. they are trainable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        D : int\n",
    "            Number of tensors to be normalized.\n",
    "        decay : float\n",
    "            Decay (momentum) for the moving mean and the moving variance.\n",
    "        eps : float\n",
    "            A small float number to avoid dividing by 0.\n",
    "        use_gamma : bool\n",
    "            Use gamma in batchnorm or not.\n",
    "        use_beta : bool\n",
    "            Use beta in batchnorm or not.\n",
    "        name : str\n",
    "            Name of this layer.\n",
    "        mean : float\n",
    "            Batch mean value. Used for initialization mean with pretrained value.\n",
    "        var : float\n",
    "            Batch variance value. Used for initialization variance with pretrained value.\n",
    "        gamma : float\n",
    "            Batchnorm gamma value. Used for initialization gamma with pretrained value.\n",
    "        beta : float\n",
    "            Batchnorm beta value. Used for initialization beta with pretrained value.\n",
    "        \"\"\"\n",
    "        super().__init__(D=D, decay=decay, eps=eps, name=name, use_gamma=use_gamma, use_beta=use_beta,\n",
    "                         type_norm='Batch', mean=mean, var=var, gamma=gamma, beta=beta)\n",
    "\n",
    "    def _init_train_params(self, data):\n",
    "        if self.running_mean is None:\n",
    "            self.running_mean = np.zeros(self.D)\n",
    "        if self.running_variance is None:\n",
    "            self.running_variance = np.ones(self.D)\n",
    "\n",
    "        name = str(self._name)\n",
    "\n",
    "        self.name_mean = 'BatchMean_{}_id_'.format(self.D) + name\n",
    "        self.name_var = 'BatchVar_{}_id_'.format(self.D) + name\n",
    "\n",
    "        self.running_mean = tf.Variable(self.running_mean, trainable=False, name=self.name_mean, dtype=tf.float32)\n",
    "        self._named_params_dict[self.name_mean] = self.running_mean\n",
    "\n",
    "        self.running_variance = tf.Variable(self.running_variance, trainable=False,\n",
    "                                            name=self.name_var, dtype=tf.float32)\n",
    "        self._named_params_dict[self.name_var] = self.running_variance\n",
    "\n",
    "    def _forward(self, X):\n",
    "        return tf.nn.batch_normalization(\n",
    "            X,\n",
    "            self.running_mean,\n",
    "            self.running_variance,\n",
    "            self.beta,\n",
    "            self.gamma,\n",
    "            self.eps\n",
    "        )\n",
    "\n",
    "    def _training_forward(self, X):\n",
    "        # These if statements check if we do batchnorm for convolution or dense\n",
    "        if len(X.shape) == 4:\n",
    "            # conv\n",
    "            axes = [0, 1, 2]\n",
    "        else:\n",
    "            # dense\n",
    "            axes = [0]\n",
    "\n",
    "        batch_mean, batch_var = tf.nn.moments(X, axes=axes)\n",
    "        update_running_mean = tf.assign(\n",
    "            self.running_mean,\n",
    "            self.running_mean * self.decay + batch_mean * (1 - self.decay)\n",
    "        )\n",
    "        update_running_variance = tf.assign(\n",
    "            self.running_variance,\n",
    "            self.running_variance * self.decay + batch_var * (1 - self.decay)\n",
    "        )\n",
    "        with tf.control_dependencies([update_running_mean, update_running_variance]):\n",
    "            out = tf.nn.batch_normalization(\n",
    "                X,\n",
    "                batch_mean,\n",
    "                batch_var,\n",
    "                self.beta,\n",
    "                self.gamma,\n",
    "                self.eps\n",
    "            )\n",
    "\n",
    "        return out\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'type': 'BatchNormLayer',\n",
    "            'params': {\n",
    "                'name': self._name,\n",
    "                'D': self.D,\n",
    "                'decay': self.decay,\n",
    "                'eps': self.eps,\n",
    "                'use_beta': self.use_beta,\n",
    "                'use_gamma': self.use_gamma,\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from makizoo.backbones.densenet.blocks import transition_layer, dense_block\n",
    "from makizoo.backbones.densenet.utils import get_batchnorm_params\n",
    "\n",
    "from makiflow.layers import *\n",
    "from makiflow.models import Classificator\n",
    "\n",
    "\n",
    "def build_DenseNet(in_x,\n",
    "        input_shape,\n",
    "        nb_layers=[6,12,24,16],\n",
    "        depth=121,\n",
    "        include_top=False,\n",
    "        num_classes=1000,\n",
    "        use_bias=False,\n",
    "        use_bottleneck=True,\n",
    "        subsample_initial_block=True,\n",
    "        activation=tf.nn.relu,\n",
    "        create_model=False,\n",
    "        name_model='MakiClassificator',\n",
    "        growth_rate=32,\n",
    "        reduction=0.0,\n",
    "        nb_blocks=3,\n",
    "        dropout_p_keep=None,\n",
    "        bn_params={},\n",
    "        prefix=1000\n",
    "    ):\n",
    "    \"\"\"\n",
    "     Parameters\n",
    "     ----------\n",
    "     x : MakiTensor\n",
    "         Input MakiTensor.\n",
    "     nb_layers : int\n",
    "         List of lenght 4, where `nb_layers[i]` is number of repetition layers at stage `i` (i from 0 to 3).\n",
    "    depth : int\n",
    "        If list `nb_layers` will be empty, number of blocks will be calculated based on `depth`\n",
    "        For example: if depth=40, then first 4 blocks will repeat repeat_times = int((depth - 4) / 3), if use_bottleneck is True when\n",
    "        repeat_times divide by 2, in these example repeat_times = 6, so `nb_layers` = [6,6,6,6] in these example.\n",
    "    growth_rate : int\n",
    "        Coefficient `k` from original papep, https://arxiv.org/pdf/1608.06993.pdf .\n",
    "    dropout_p_keep : float\n",
    "        The probability that each element of x is not discarded.\n",
    "    reduction : float\n",
    "        Coefficient, where `r` = 1 - `rediction`, `r` is how much number of feature maps need to compress in transition layers.\n",
    "    input_shape : List\n",
    "        Input shape of neural network. Example - [32, 128, 128, 3]\n",
    "        which mean 32 - batch size, two 128 - size of picture, 3 - number of colors.\n",
    "    create_model : bool\n",
    "        Return build classification model, otherwise return input MakiTensor and output MakiTensor.\n",
    "    name_model : str\n",
    "        Name of model, if it will be created.\n",
    "    nb_blocks : int\n",
    "        Number of dense blocks.\n",
    "    num_classes : int\n",
    "        Number of classes that you need to classify.\n",
    "    bn_params : dict\n",
    "        Parameters for BatchNormLayer. If empty all parameters will have default valued.\n",
    "    include_top : bool\n",
    "        If true when at the end of the neural network added Global Avg pooling and Dense Layer without\n",
    "        activation with the number of output neurons equal to num_classes.\n",
    "    activation : tensorflow function\n",
    "        The function of activation, by default tf.nn.relu.\n",
    "\n",
    "     Returns\n",
    "     ---------\n",
    "     in_x : MakiTensor\n",
    "         Input MakiTensor.\n",
    "     output : int\n",
    "         Output MakiTensor.\n",
    "     Classificator : MakiFlow.Classificator\n",
    "         Constructed model.\n",
    "     \"\"\"\n",
    "    if bn_params is None or len(bn_params) == 0:\n",
    "        bn_params = get_batchnorm_params()\n",
    "    compression = 1 - reduction\n",
    "\n",
    "    if len(nb_layers) == 0:\n",
    "        count = int((depth - 4) / 3)\n",
    "        nb_layers = [count for _ in range(nb_blocks+1)]\n",
    "\n",
    "    if subsample_initial_block:\n",
    "        x = ZeroPaddingLayer(padding=[[3,3],[3,3]], name='zero_padding2d_4' + str(prefix))(in_x)\n",
    "\n",
    "        x = ConvLayer(kw=7,kh=7,in_f=3, stride=2, out_f=growth_rate * 2, activation=None, use_bias=use_bias,\n",
    "                name='conv1/conv' + str(prefix), padding='VALID')(x)\n",
    "\n",
    "        x = BatchNormLayer(D=growth_rate * 2, name='conv1/bn' + str(prefix), **bn_params)(x)\n",
    "        x = ActivationLayer(activation=activation, name='conv1/relu' + str(prefix))(x)\n",
    "        x = ZeroPaddingLayer(padding=[[1,1],[1,1]], name='zero_padding2d_5' + str(prefix))(x)\n",
    "\n",
    "        x = MaxPoolLayer(ksize=[1,3,3,1], padding='VALID', name='pool1' + str(prefix))(x)\n",
    "    else:\n",
    "        x = ConvLayer(kw=3,kh=3,in_f=3, stride=1, out_f=growth_rate * 2, activation=None, use_bias=use_bias,\n",
    "                name='conv1/conv' + str(prefix))(in_x)\n",
    "\n",
    "    # densenet blocks\n",
    "    for block_index in range(len(nb_layers) - 1):\n",
    "        # dense block\n",
    "        x = dense_block(x=x, nb_layers=nb_layers[block_index], stage=block_index+2+int(prefix),\n",
    "                growth_rate=growth_rate, dropout_p_keep=dropout_p_keep, use_bottleneck=use_bottleneck,\n",
    "                        activation=activation, use_bias=use_bias, bn_params=bn_params)\n",
    "\n",
    "        # transition block\n",
    "        x = transition_layer(x=x,\n",
    "             dropout_p_keep=dropout_p_keep, number=block_index+2+int(prefix), compression=compression,\n",
    "                        activation=activation, use_bias=use_bias, bn_params=bn_params)\n",
    "\n",
    "    x = dense_block(x=x, nb_layers=nb_layers[-1], stage=len(nb_layers)+1+int(prefix),\n",
    "            growth_rate=growth_rate, dropout_p_keep=dropout_p_keep, use_bottleneck=use_bottleneck,\n",
    "                        activation=activation, use_bias=use_bias, bn_params=bn_params)\n",
    "\n",
    "    x = BatchNormLayer(D=x.get_shape()[-1], name='bn'  + str(prefix), **bn_params)(x)\n",
    "    x = ActivationLayer(activation=activation, name='relu' + str(prefix))(x)\n",
    "    if include_top:\n",
    "        x = GlobalAvgPoolLayer(name='avg_pool')(x)\n",
    "        # dense part (fc layers)\n",
    "        output = DenseLayer(in_d=x.get_shape()[-1], out_d=num_classes, activation=None, use_bias=True, name=\"fc1000\")(x)\n",
    "        if create_model:\n",
    "            return Classificator(in_x, output, name_model)\n",
    "    else:\n",
    "        output = x\n",
    "\n",
    "    return GlobalAvgPoolLayer(name='avg_pool'  + str(prefix))(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=32\n",
    "\n",
    "def preprocess_densenet(x,prefix):\n",
    "    return build_DenseNet(in_x=x,\n",
    "        input_shape=[batch_size,image_size,image_size,3],\n",
    "        nb_layers=[],\n",
    "        depth=40,\n",
    "        include_top=False,\n",
    "        num_classes=10,\n",
    "        use_bias=False,\n",
    "        use_bottleneck=True,\n",
    "        subsample_initial_block=False,\n",
    "        activation=tf.nn.relu,\n",
    "        create_model=False,\n",
    "        name_model='densenet-100',\n",
    "        growth_rate=24,\n",
    "        reduction=0.7,\n",
    "        nb_blocks=3,\n",
    "        dropout_p_keep=0.8,\n",
    "        bn_params={},\n",
    "        prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_x1 = InputLayer(input_shape=[batch_size,None,None,3],name='Input1')\n",
    "in_x2 = InputLayer(input_shape=[batch_size,None,None,3],name='Input2')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = preprocess_densenet(in_x1,prefix=20000)\n",
    "out2 = preprocess_densenet(in_x2,prefix=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 409]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Input1',\n",
       " 'conv1/conv20000',\n",
       " 'conv20002_block1_0_bn',\n",
       " 'conv20002_block1_0_relu',\n",
       " 'conv20002_block1_1_conv',\n",
       " 'conv20002_block1_1_dropout',\n",
       " 'conv20002_block1_1_bn',\n",
       " 'conv20002_block1_1_relu',\n",
       " 'conv20002_block1_2_conv',\n",
       " 'conv20002_block1_2_dropout',\n",
       " 'conv20002_block1_concat',\n",
       " 'conv20002_block2_0_bn',\n",
       " 'conv20002_block2_0_relu',\n",
       " 'conv20002_block2_1_conv',\n",
       " 'conv20002_block2_1_dropout',\n",
       " 'conv20002_block2_1_bn',\n",
       " 'conv20002_block2_1_relu',\n",
       " 'conv20002_block2_2_conv',\n",
       " 'conv20002_block2_2_dropout',\n",
       " 'conv20002_block2_concat',\n",
       " 'conv20002_block3_0_bn',\n",
       " 'conv20002_block3_0_relu',\n",
       " 'conv20002_block3_1_conv',\n",
       " 'conv20002_block3_1_dropout',\n",
       " 'conv20002_block3_1_bn',\n",
       " 'conv20002_block3_1_relu',\n",
       " 'conv20002_block3_2_conv',\n",
       " 'conv20002_block3_2_dropout',\n",
       " 'conv20002_block3_concat',\n",
       " 'conv20002_block4_0_bn',\n",
       " 'conv20002_block4_0_relu',\n",
       " 'conv20002_block4_1_conv',\n",
       " 'conv20002_block4_1_dropout',\n",
       " 'conv20002_block4_1_bn',\n",
       " 'conv20002_block4_1_relu',\n",
       " 'conv20002_block4_2_conv',\n",
       " 'conv20002_block4_2_dropout',\n",
       " 'conv20002_block4_concat',\n",
       " 'conv20002_block5_0_bn',\n",
       " 'conv20002_block5_0_relu',\n",
       " 'conv20002_block5_1_conv',\n",
       " 'conv20002_block5_1_dropout',\n",
       " 'conv20002_block5_1_bn',\n",
       " 'conv20002_block5_1_relu',\n",
       " 'conv20002_block5_2_conv',\n",
       " 'conv20002_block5_2_dropout',\n",
       " 'conv20002_block5_concat',\n",
       " 'conv20002_block6_0_bn',\n",
       " 'conv20002_block6_0_relu',\n",
       " 'conv20002_block6_1_conv',\n",
       " 'conv20002_block6_1_dropout',\n",
       " 'conv20002_block6_1_bn',\n",
       " 'conv20002_block6_1_relu',\n",
       " 'conv20002_block6_2_conv',\n",
       " 'conv20002_block6_2_dropout',\n",
       " 'conv20002_block6_concat',\n",
       " 'conv20002_block7_0_bn',\n",
       " 'conv20002_block7_0_relu',\n",
       " 'conv20002_block7_1_conv',\n",
       " 'conv20002_block7_1_dropout',\n",
       " 'conv20002_block7_1_bn',\n",
       " 'conv20002_block7_1_relu',\n",
       " 'conv20002_block7_2_conv',\n",
       " 'conv20002_block7_2_dropout',\n",
       " 'conv20002_block7_concat',\n",
       " 'conv20002_block8_0_bn',\n",
       " 'conv20002_block8_0_relu',\n",
       " 'conv20002_block8_1_conv',\n",
       " 'conv20002_block8_1_dropout',\n",
       " 'conv20002_block8_1_bn',\n",
       " 'conv20002_block8_1_relu',\n",
       " 'conv20002_block8_2_conv',\n",
       " 'conv20002_block8_2_dropout',\n",
       " 'conv20002_block8_concat',\n",
       " 'conv20002_block9_0_bn',\n",
       " 'conv20002_block9_0_relu',\n",
       " 'conv20002_block9_1_conv',\n",
       " 'conv20002_block9_1_dropout',\n",
       " 'conv20002_block9_1_bn',\n",
       " 'conv20002_block9_1_relu',\n",
       " 'conv20002_block9_2_conv',\n",
       " 'conv20002_block9_2_dropout',\n",
       " 'conv20002_block9_concat',\n",
       " 'conv20002_block10_0_bn',\n",
       " 'conv20002_block10_0_relu',\n",
       " 'conv20002_block10_1_conv',\n",
       " 'conv20002_block10_1_dropout',\n",
       " 'conv20002_block10_1_bn',\n",
       " 'conv20002_block10_1_relu',\n",
       " 'conv20002_block10_2_conv',\n",
       " 'conv20002_block10_2_dropout',\n",
       " 'conv20002_block10_concat',\n",
       " 'conv20002_block11_0_bn',\n",
       " 'conv20002_block11_0_relu',\n",
       " 'conv20002_block11_1_conv',\n",
       " 'conv20002_block11_1_dropout',\n",
       " 'conv20002_block11_1_bn',\n",
       " 'conv20002_block11_1_relu',\n",
       " 'conv20002_block11_2_conv',\n",
       " 'conv20002_block11_2_dropout',\n",
       " 'conv20002_block11_concat',\n",
       " 'conv20002_block12_0_bn',\n",
       " 'conv20002_block12_0_relu',\n",
       " 'conv20002_block12_1_conv',\n",
       " 'conv20002_block12_1_dropout',\n",
       " 'conv20002_block12_1_bn',\n",
       " 'conv20002_block12_1_relu',\n",
       " 'conv20002_block12_2_conv',\n",
       " 'conv20002_block12_2_dropout',\n",
       " 'conv20002_block12_concat',\n",
       " 'pool20002_bn',\n",
       " 'pool20002_relu',\n",
       " 'pool20002_conv',\n",
       " 'pool20002_dropout',\n",
       " 'pool20002_avg_pool',\n",
       " 'conv20003_block1_0_bn',\n",
       " 'conv20003_block1_0_relu',\n",
       " 'conv20003_block1_1_conv',\n",
       " 'conv20003_block1_1_dropout',\n",
       " 'conv20003_block1_1_bn',\n",
       " 'conv20003_block1_1_relu',\n",
       " 'conv20003_block1_2_conv',\n",
       " 'conv20003_block1_2_dropout',\n",
       " 'conv20003_block1_concat',\n",
       " 'conv20003_block2_0_bn',\n",
       " 'conv20003_block2_0_relu',\n",
       " 'conv20003_block2_1_conv',\n",
       " 'conv20003_block2_1_dropout',\n",
       " 'conv20003_block2_1_bn',\n",
       " 'conv20003_block2_1_relu',\n",
       " 'conv20003_block2_2_conv',\n",
       " 'conv20003_block2_2_dropout',\n",
       " 'conv20003_block2_concat',\n",
       " 'conv20003_block3_0_bn',\n",
       " 'conv20003_block3_0_relu',\n",
       " 'conv20003_block3_1_conv',\n",
       " 'conv20003_block3_1_dropout',\n",
       " 'conv20003_block3_1_bn',\n",
       " 'conv20003_block3_1_relu',\n",
       " 'conv20003_block3_2_conv',\n",
       " 'conv20003_block3_2_dropout',\n",
       " 'conv20003_block3_concat',\n",
       " 'conv20003_block4_0_bn',\n",
       " 'conv20003_block4_0_relu',\n",
       " 'conv20003_block4_1_conv',\n",
       " 'conv20003_block4_1_dropout',\n",
       " 'conv20003_block4_1_bn',\n",
       " 'conv20003_block4_1_relu',\n",
       " 'conv20003_block4_2_conv',\n",
       " 'conv20003_block4_2_dropout',\n",
       " 'conv20003_block4_concat',\n",
       " 'conv20003_block5_0_bn',\n",
       " 'conv20003_block5_0_relu',\n",
       " 'conv20003_block5_1_conv',\n",
       " 'conv20003_block5_1_dropout',\n",
       " 'conv20003_block5_1_bn',\n",
       " 'conv20003_block5_1_relu',\n",
       " 'conv20003_block5_2_conv',\n",
       " 'conv20003_block5_2_dropout',\n",
       " 'conv20003_block5_concat',\n",
       " 'conv20003_block6_0_bn',\n",
       " 'conv20003_block6_0_relu',\n",
       " 'conv20003_block6_1_conv',\n",
       " 'conv20003_block6_1_dropout',\n",
       " 'conv20003_block6_1_bn',\n",
       " 'conv20003_block6_1_relu',\n",
       " 'conv20003_block6_2_conv',\n",
       " 'conv20003_block6_2_dropout',\n",
       " 'conv20003_block6_concat',\n",
       " 'conv20003_block7_0_bn',\n",
       " 'conv20003_block7_0_relu',\n",
       " 'conv20003_block7_1_conv',\n",
       " 'conv20003_block7_1_dropout',\n",
       " 'conv20003_block7_1_bn',\n",
       " 'conv20003_block7_1_relu',\n",
       " 'conv20003_block7_2_conv',\n",
       " 'conv20003_block7_2_dropout',\n",
       " 'conv20003_block7_concat',\n",
       " 'conv20003_block8_0_bn',\n",
       " 'conv20003_block8_0_relu',\n",
       " 'conv20003_block8_1_conv',\n",
       " 'conv20003_block8_1_dropout',\n",
       " 'conv20003_block8_1_bn',\n",
       " 'conv20003_block8_1_relu',\n",
       " 'conv20003_block8_2_conv',\n",
       " 'conv20003_block8_2_dropout',\n",
       " 'conv20003_block8_concat',\n",
       " 'conv20003_block9_0_bn',\n",
       " 'conv20003_block9_0_relu',\n",
       " 'conv20003_block9_1_conv',\n",
       " 'conv20003_block9_1_dropout',\n",
       " 'conv20003_block9_1_bn',\n",
       " 'conv20003_block9_1_relu',\n",
       " 'conv20003_block9_2_conv',\n",
       " 'conv20003_block9_2_dropout',\n",
       " 'conv20003_block9_concat',\n",
       " 'conv20003_block10_0_bn',\n",
       " 'conv20003_block10_0_relu',\n",
       " 'conv20003_block10_1_conv',\n",
       " 'conv20003_block10_1_dropout',\n",
       " 'conv20003_block10_1_bn',\n",
       " 'conv20003_block10_1_relu',\n",
       " 'conv20003_block10_2_conv',\n",
       " 'conv20003_block10_2_dropout',\n",
       " 'conv20003_block10_concat',\n",
       " 'conv20003_block11_0_bn',\n",
       " 'conv20003_block11_0_relu',\n",
       " 'conv20003_block11_1_conv',\n",
       " 'conv20003_block11_1_dropout',\n",
       " 'conv20003_block11_1_bn',\n",
       " 'conv20003_block11_1_relu',\n",
       " 'conv20003_block11_2_conv',\n",
       " 'conv20003_block11_2_dropout',\n",
       " 'conv20003_block11_concat',\n",
       " 'conv20003_block12_0_bn',\n",
       " 'conv20003_block12_0_relu',\n",
       " 'conv20003_block12_1_conv',\n",
       " 'conv20003_block12_1_dropout',\n",
       " 'conv20003_block12_1_bn',\n",
       " 'conv20003_block12_1_relu',\n",
       " 'conv20003_block12_2_conv',\n",
       " 'conv20003_block12_2_dropout',\n",
       " 'conv20003_block12_concat',\n",
       " 'pool20003_bn',\n",
       " 'pool20003_relu',\n",
       " 'pool20003_conv',\n",
       " 'pool20003_dropout',\n",
       " 'pool20003_avg_pool',\n",
       " 'conv20004_block1_0_bn',\n",
       " 'conv20004_block1_0_relu',\n",
       " 'conv20004_block1_1_conv',\n",
       " 'conv20004_block1_1_dropout',\n",
       " 'conv20004_block1_1_bn',\n",
       " 'conv20004_block1_1_relu',\n",
       " 'conv20004_block1_2_conv',\n",
       " 'conv20004_block1_2_dropout',\n",
       " 'conv20004_block1_concat',\n",
       " 'conv20004_block2_0_bn',\n",
       " 'conv20004_block2_0_relu',\n",
       " 'conv20004_block2_1_conv',\n",
       " 'conv20004_block2_1_dropout',\n",
       " 'conv20004_block2_1_bn',\n",
       " 'conv20004_block2_1_relu',\n",
       " 'conv20004_block2_2_conv',\n",
       " 'conv20004_block2_2_dropout',\n",
       " 'conv20004_block2_concat',\n",
       " 'conv20004_block3_0_bn',\n",
       " 'conv20004_block3_0_relu',\n",
       " 'conv20004_block3_1_conv',\n",
       " 'conv20004_block3_1_dropout',\n",
       " 'conv20004_block3_1_bn',\n",
       " 'conv20004_block3_1_relu',\n",
       " 'conv20004_block3_2_conv',\n",
       " 'conv20004_block3_2_dropout',\n",
       " 'conv20004_block3_concat',\n",
       " 'conv20004_block4_0_bn',\n",
       " 'conv20004_block4_0_relu',\n",
       " 'conv20004_block4_1_conv',\n",
       " 'conv20004_block4_1_dropout',\n",
       " 'conv20004_block4_1_bn',\n",
       " 'conv20004_block4_1_relu',\n",
       " 'conv20004_block4_2_conv',\n",
       " 'conv20004_block4_2_dropout',\n",
       " 'conv20004_block4_concat',\n",
       " 'conv20004_block5_0_bn',\n",
       " 'conv20004_block5_0_relu',\n",
       " 'conv20004_block5_1_conv',\n",
       " 'conv20004_block5_1_dropout',\n",
       " 'conv20004_block5_1_bn',\n",
       " 'conv20004_block5_1_relu',\n",
       " 'conv20004_block5_2_conv',\n",
       " 'conv20004_block5_2_dropout',\n",
       " 'conv20004_block5_concat',\n",
       " 'conv20004_block6_0_bn',\n",
       " 'conv20004_block6_0_relu',\n",
       " 'conv20004_block6_1_conv',\n",
       " 'conv20004_block6_1_dropout',\n",
       " 'conv20004_block6_1_bn',\n",
       " 'conv20004_block6_1_relu',\n",
       " 'conv20004_block6_2_conv',\n",
       " 'conv20004_block6_2_dropout',\n",
       " 'conv20004_block6_concat',\n",
       " 'conv20004_block7_0_bn',\n",
       " 'conv20004_block7_0_relu',\n",
       " 'conv20004_block7_1_conv',\n",
       " 'conv20004_block7_1_dropout',\n",
       " 'conv20004_block7_1_bn',\n",
       " 'conv20004_block7_1_relu',\n",
       " 'conv20004_block7_2_conv',\n",
       " 'conv20004_block7_2_dropout',\n",
       " 'conv20004_block7_concat',\n",
       " 'conv20004_block8_0_bn',\n",
       " 'conv20004_block8_0_relu',\n",
       " 'conv20004_block8_1_conv',\n",
       " 'conv20004_block8_1_dropout',\n",
       " 'conv20004_block8_1_bn',\n",
       " 'conv20004_block8_1_relu',\n",
       " 'conv20004_block8_2_conv',\n",
       " 'conv20004_block8_2_dropout',\n",
       " 'conv20004_block8_concat',\n",
       " 'conv20004_block9_0_bn',\n",
       " 'conv20004_block9_0_relu',\n",
       " 'conv20004_block9_1_conv',\n",
       " 'conv20004_block9_1_dropout',\n",
       " 'conv20004_block9_1_bn',\n",
       " 'conv20004_block9_1_relu',\n",
       " 'conv20004_block9_2_conv',\n",
       " 'conv20004_block9_2_dropout',\n",
       " 'conv20004_block9_concat',\n",
       " 'conv20004_block10_0_bn',\n",
       " 'conv20004_block10_0_relu',\n",
       " 'conv20004_block10_1_conv',\n",
       " 'conv20004_block10_1_dropout',\n",
       " 'conv20004_block10_1_bn',\n",
       " 'conv20004_block10_1_relu',\n",
       " 'conv20004_block10_2_conv',\n",
       " 'conv20004_block10_2_dropout',\n",
       " 'conv20004_block10_concat',\n",
       " 'conv20004_block11_0_bn',\n",
       " 'conv20004_block11_0_relu',\n",
       " 'conv20004_block11_1_conv',\n",
       " 'conv20004_block11_1_dropout',\n",
       " 'conv20004_block11_1_bn',\n",
       " 'conv20004_block11_1_relu',\n",
       " 'conv20004_block11_2_conv',\n",
       " 'conv20004_block11_2_dropout',\n",
       " 'conv20004_block11_concat',\n",
       " 'conv20004_block12_0_bn',\n",
       " 'conv20004_block12_0_relu',\n",
       " 'conv20004_block12_1_conv',\n",
       " 'conv20004_block12_1_dropout',\n",
       " 'conv20004_block12_1_bn',\n",
       " 'conv20004_block12_1_relu',\n",
       " 'conv20004_block12_2_conv',\n",
       " 'conv20004_block12_2_dropout',\n",
       " 'conv20004_block12_concat',\n",
       " 'pool20004_bn',\n",
       " 'pool20004_relu',\n",
       " 'pool20004_conv',\n",
       " 'pool20004_dropout',\n",
       " 'pool20004_avg_pool',\n",
       " 'conv20005_block1_0_bn',\n",
       " 'conv20005_block1_0_relu',\n",
       " 'conv20005_block1_1_conv',\n",
       " 'conv20005_block1_1_dropout',\n",
       " 'conv20005_block1_1_bn',\n",
       " 'conv20005_block1_1_relu',\n",
       " 'conv20005_block1_2_conv',\n",
       " 'conv20005_block1_2_dropout',\n",
       " 'conv20005_block1_concat',\n",
       " 'conv20005_block2_0_bn',\n",
       " 'conv20005_block2_0_relu',\n",
       " 'conv20005_block2_1_conv',\n",
       " 'conv20005_block2_1_dropout',\n",
       " 'conv20005_block2_1_bn',\n",
       " 'conv20005_block2_1_relu',\n",
       " 'conv20005_block2_2_conv',\n",
       " 'conv20005_block2_2_dropout',\n",
       " 'conv20005_block2_concat',\n",
       " 'conv20005_block3_0_bn',\n",
       " 'conv20005_block3_0_relu',\n",
       " 'conv20005_block3_1_conv',\n",
       " 'conv20005_block3_1_dropout',\n",
       " 'conv20005_block3_1_bn',\n",
       " 'conv20005_block3_1_relu',\n",
       " 'conv20005_block3_2_conv',\n",
       " 'conv20005_block3_2_dropout',\n",
       " 'conv20005_block3_concat',\n",
       " 'conv20005_block4_0_bn',\n",
       " 'conv20005_block4_0_relu',\n",
       " 'conv20005_block4_1_conv',\n",
       " 'conv20005_block4_1_dropout',\n",
       " 'conv20005_block4_1_bn',\n",
       " 'conv20005_block4_1_relu',\n",
       " 'conv20005_block4_2_conv',\n",
       " 'conv20005_block4_2_dropout',\n",
       " 'conv20005_block4_concat',\n",
       " 'conv20005_block5_0_bn',\n",
       " 'conv20005_block5_0_relu',\n",
       " 'conv20005_block5_1_conv',\n",
       " 'conv20005_block5_1_dropout',\n",
       " 'conv20005_block5_1_bn',\n",
       " 'conv20005_block5_1_relu',\n",
       " 'conv20005_block5_2_conv',\n",
       " 'conv20005_block5_2_dropout',\n",
       " 'conv20005_block5_concat',\n",
       " 'conv20005_block6_0_bn',\n",
       " 'conv20005_block6_0_relu',\n",
       " 'conv20005_block6_1_conv',\n",
       " 'conv20005_block6_1_dropout',\n",
       " 'conv20005_block6_1_bn',\n",
       " 'conv20005_block6_1_relu',\n",
       " 'conv20005_block6_2_conv',\n",
       " 'conv20005_block6_2_dropout',\n",
       " 'conv20005_block6_concat',\n",
       " 'conv20005_block7_0_bn',\n",
       " 'conv20005_block7_0_relu',\n",
       " 'conv20005_block7_1_conv',\n",
       " 'conv20005_block7_1_dropout',\n",
       " 'conv20005_block7_1_bn',\n",
       " 'conv20005_block7_1_relu',\n",
       " 'conv20005_block7_2_conv',\n",
       " 'conv20005_block7_2_dropout',\n",
       " 'conv20005_block7_concat',\n",
       " 'conv20005_block8_0_bn',\n",
       " 'conv20005_block8_0_relu',\n",
       " 'conv20005_block8_1_conv',\n",
       " 'conv20005_block8_1_dropout',\n",
       " 'conv20005_block8_1_bn',\n",
       " 'conv20005_block8_1_relu',\n",
       " 'conv20005_block8_2_conv',\n",
       " 'conv20005_block8_2_dropout',\n",
       " 'conv20005_block8_concat',\n",
       " 'conv20005_block9_0_bn',\n",
       " 'conv20005_block9_0_relu',\n",
       " 'conv20005_block9_1_conv',\n",
       " 'conv20005_block9_1_dropout',\n",
       " 'conv20005_block9_1_bn',\n",
       " 'conv20005_block9_1_relu',\n",
       " 'conv20005_block9_2_conv',\n",
       " 'conv20005_block9_2_dropout',\n",
       " 'conv20005_block9_concat',\n",
       " 'conv20005_block10_0_bn',\n",
       " 'conv20005_block10_0_relu',\n",
       " 'conv20005_block10_1_conv',\n",
       " 'conv20005_block10_1_dropout',\n",
       " 'conv20005_block10_1_bn',\n",
       " 'conv20005_block10_1_relu',\n",
       " 'conv20005_block10_2_conv',\n",
       " 'conv20005_block10_2_dropout',\n",
       " 'conv20005_block10_concat',\n",
       " 'conv20005_block11_0_bn',\n",
       " 'conv20005_block11_0_relu',\n",
       " 'conv20005_block11_1_conv',\n",
       " 'conv20005_block11_1_dropout',\n",
       " 'conv20005_block11_1_bn',\n",
       " 'conv20005_block11_1_relu',\n",
       " 'conv20005_block11_2_conv',\n",
       " 'conv20005_block11_2_dropout',\n",
       " 'conv20005_block11_concat',\n",
       " 'conv20005_block12_0_bn',\n",
       " 'conv20005_block12_0_relu',\n",
       " 'conv20005_block12_1_conv',\n",
       " 'conv20005_block12_1_dropout',\n",
       " 'conv20005_block12_1_bn',\n",
       " 'conv20005_block12_1_relu',\n",
       " 'conv20005_block12_2_conv',\n",
       " 'conv20005_block12_2_dropout',\n",
       " 'conv20005_block12_concat',\n",
       " 'bn20000',\n",
       " 'relu20000',\n",
       " 'avg_pool20000']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = list(out1.get_previous_tensors()) + [out1.get_name()]\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = ConcatLayer(name='concat', axis=-1)([out1, out2])\n",
    "#concat = SumLayer(name='sum')([out1, out2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 818]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = FlattenLayer('flatten1')(x)\n",
    "\n",
    "#dense1 = DenseLayer(in_d=1200, out_d=1200, activation=None, name='dense1')(concat)\n",
    "#dense1 = BatchNormLayer(D=546, name='bn2_dense')(dense1)\n",
    "#dense1 = DropoutLayer(p_keep=0.9, name='drop1')(dense1)\n",
    "#dense1 = ActivationLayer('act1_dense', activation=tf.nn.relu)(dense1)\n",
    "\n",
    "#dense2 = DenseLayer(in_d=1200, out_d=1200, activation=None, name='dense2')(dense1)\n",
    "#dense2 = BatchNormLayer(D=546, name='bn2_dense')(dense2)\n",
    "#dense2 = DropoutLayer(p_keep=0.9, name='drop2')(dense2)\n",
    "#dense2 = ActivationLayer('act2_dense', activation=tf.nn.relu)(dense2)\n",
    "\n",
    "#dense6 = DenseLayer(in_d=179, out_d=179, activation=None,name='dense6')(concat)\n",
    "#dense6 = BatchNormLayer(D=179, name='bn4')(dense6)\n",
    "#dense6 = ActivationLayer('act56_dense', activation=tf.nn.relu)(dense6)\n",
    "#dense6 = DropoutLayer(p_keep=0.8, name='drop6_dense')(concat)\n",
    "dense7 = DenseLayer(in_d=818, out_d=1, activation=None, name='criteria')(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 818]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy \n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Regressor(MakiModel):\n",
    "    def __init__(self, in_x1, in_x2, out_x, name='Regressor'):\n",
    "        graph_tensors = copy(out_x.get_previous_tensors())\n",
    "        # Add output tensor to `graph_tensors` since it doesn't have it.\n",
    "        # It is assumed that graph_tensors contains ALL THE TENSORS graph consists of.\n",
    "        graph_tensors.update(out_x.get_self_pair())\n",
    "        outputs = [out_x]\n",
    "        inputs = [in_x1, in_x2]\n",
    "        super().__init__(graph_tensors, outputs, inputs)\n",
    "        self.name = str(name)\n",
    "        self._batch_sz = in_x1.get_shape()[0]\n",
    "        self._images1 = self._input_data_tensors[0]\n",
    "        self._images2 = self._input_data_tensors[1]\n",
    "        self._inference_out = self._output_data_tensors[0]\n",
    "        # For training\n",
    "        self._training_vars_are_ready = False\n",
    "\n",
    "    def _prepare_training_vars(self):\n",
    "        if not self._set_for_training:\n",
    "            super()._setup_for_training()\n",
    "\n",
    "        self._logits = self._training_outputs[0]\n",
    "        self._num_classes = self._logits.get_shape()[-1]\n",
    "        self._labels = tf.placeholder(tf.float32, shape=[self._batch_sz, 1])\n",
    "        \n",
    "        self._training_vars_are_ready = True\n",
    "        self._ce_loss_is_build =False\n",
    "        \n",
    "    def _build_ce_loss(self):\n",
    "        self._final_ce_loss = tf.reduce_mean(\n",
    "            tf.abs(self._labels - self._logits)\n",
    "        )\n",
    "\n",
    "        self._ce_loss_is_build = True\n",
    "\n",
    "    def _minimize_ce_loss(self, optimizer, global_step):\n",
    "        if not self._set_for_training:\n",
    "            super()._setup_for_training()\n",
    "\n",
    "        if not self._training_vars_are_ready:\n",
    "            self._prepare_training_vars()\n",
    "\n",
    "        if not self._ce_loss_is_build:\n",
    "            # no need to setup any inputs for this loss\n",
    "            self._build_ce_loss()\n",
    "            self._ce_optimizer = optimizer\n",
    "            self._ce_train_op = optimizer.minimize(\n",
    "                self._final_ce_loss, var_list=self._trainable_vars, global_step=global_step\n",
    "            )\n",
    "            self._session.run(tf.variables_initializer(optimizer.variables()))\n",
    "\n",
    "        if self._ce_optimizer != optimizer:\n",
    "            print('New optimizer is used.')\n",
    "            self._ce_optimizer = optimizer\n",
    "            self._ce_train_op = optimizer.minimize(\n",
    "                self._final_ce_loss, var_list=self._trainable_vars, global_step=global_step\n",
    "            )\n",
    "            self._session.run(tf.variables_initializer(optimizer.variables()))\n",
    "\n",
    "        return self._ce_train_op\n",
    "\n",
    "    def fit_ce(\n",
    "            self, Xtraina, Xtrainb, Ytrain, Xtesta, Xtestb, Ytest, optimizer=None, epochs=1, global_step=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Method for training the model. Works faster than `verbose_fit` method because\n",
    "        it uses exponential decay in order to speed up training. It produces less accurate\n",
    "        train error mesurement.\n",
    "        Parameters\n",
    "        ----------\n",
    "            Xtrain : numpy array\n",
    "                Training images stacked into one big array with shape (num_images, image_w, image_h, image_depth).\n",
    "            Ytrain : numpy array\n",
    "                Training label for each image in `Xtrain` array with shape (num_images).\n",
    "                IMPORTANT: ALL LABELS MUST BE NOT ONE-HOT ENCODED, USE SPARSE TRAINING DATA INSTEAD.\n",
    "            Xtest : numpy array\n",
    "                Same as `Xtrain` but for testing.\n",
    "            Ytest : numpy array\n",
    "                Same as `Ytrain` but for testing.\n",
    "            optimizer : tensorflow optimizer\n",
    "                Model uses tensorflow optimizers in order train itself.\n",
    "            epochs : int\n",
    "                Number of epochs.\n",
    "            test_period : int\n",
    "                Test begins each `test_period` epochs. You can set a larger number in order to\n",
    "                speed up training.\n",
    "        Returns\n",
    "        -------\n",
    "            python dictionary\n",
    "                Dictionary with all testing data(train error, train cost, test error, test cost)\n",
    "                for each test period.\n",
    "        \"\"\"\n",
    "\n",
    "        assert (optimizer is not None)\n",
    "        assert (self._session is not None)\n",
    "        train_op = self._minimize_ce_loss(optimizer, global_step)\n",
    "\n",
    "        n_batches = len(Xtraina) // self._batch_sz\n",
    "        bs = self._batch_sz\n",
    "\n",
    "        train_costs = []\n",
    "        \n",
    "        test_costs = []\n",
    "        iterator = None\n",
    "        train_cost = np.float32(0)\n",
    "        for i in range(epochs):\n",
    "            Xtraina, Xtrainb, Ytrain = shuffle(Xtraina,Xtrainb, Ytrain)\n",
    "            iterator = tqdm(range(n_batches))\n",
    "\n",
    "            for j in iterator:\n",
    "                Xbatcha = Xtraina[j*bs:(j+1)*bs]\n",
    "                Xbatchb = Xtrainb[j*bs:(j+1)*bs]\n",
    "                Ybatch = Ytrain[j*bs:(j+1)*bs]\n",
    "                train_cost_batch, _ = self._session.run(\n",
    "                    [self._final_ce_loss, train_op],\n",
    "                    feed_dict={\n",
    "                        self._images1: Xbatcha,\n",
    "                        self._images2: Xbatchb,\n",
    "                        self._labels: Ybatch\n",
    "                    })\n",
    "\n",
    "            train_costs.append(train_cost)\n",
    "            test_cost = 0\n",
    "            for j in range(len(Xtesta) // bs):\n",
    "                Xbatcha = Xtesta[j*bs:(j+1)*bs]\n",
    "                Xbatchb = Xtestb[j*bs:(j+1)*bs]\n",
    "                Ybatch = Ytest[j*bs:(j+1)*bs]\n",
    "                test_cost += self._session.run(\n",
    "                    [self._final_ce_loss],\n",
    "                    feed_dict={\n",
    "                        self._images1: Xbatcha,\n",
    "                        self._images2: Xbatchb,\n",
    "                        self._labels: Ybatch\n",
    "                    })[0]\n",
    "            test_costs.append(test_cost / (len(Xtesta) // bs))\n",
    "            train_cost = 0\n",
    "            for j in range(len(Xtraina) // bs):\n",
    "                Xbatcha = Xtraina[j*bs:(j+1)*bs]\n",
    "                Xbatchb = Xtrainb[j*bs:(j+1)*bs]\n",
    "                Ybatch = Ytrain[j*bs:(j+1)*bs]\n",
    "                train_cost += self._session.run(\n",
    "                    [self._final_ce_loss],\n",
    "                    feed_dict={\n",
    "                        self._images1: Xbatcha,\n",
    "                        self._images2: Xbatchb,\n",
    "                        self._labels: Ybatch\n",
    "                    })[0]\n",
    "            \n",
    "            print('Epoch:', i,\n",
    "                  'Train cost: {:0.4f}'.format(train_cost / (len(Xtraina) // bs)),\n",
    "                  'Test cost: {:0.4f}'.format(test_cost / (len(Xtesta) // bs)),\n",
    "                 )\n",
    "        if iterator is not None:\n",
    "            iterator.close()\n",
    "        return {'train costs': train_costs, 'test costs':test_costs}\n",
    "        \n",
    "    def predict(self, val):\n",
    "        return self._session.run(\n",
    "            self._inference_out,\n",
    "            feed_dict={\n",
    "                self._images1: val[0],\n",
    "                self._images2: val[1]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Regressor(in_x1, in_x2, out_x=dense7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_session(ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('/home/rustam/EyePit/weights/weights/densenet/121/model_densenet_121.ckpt', layer_names=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_layers_trainable([(l, False) for l in layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_input2(x):\n",
    "    #size = (x.shape[1] // 2, x.shape[0] // 2)\n",
    "    x = cv2.resize(x, (image_size, image_size)).astype(np.float32)\n",
    "    #mean = [103.939, 116.779, 123.68]\n",
    "    # x = x[...,::-1]\n",
    "    #x[..., 0] = x[..., 0] - mean[0]\n",
    "    #x[..., 1] = x[..., 1] - mean[1]\n",
    "    #x[..., 2] = x[..., 2] - mean[2]\n",
    "    x = (x / 128) - 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rustam/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n"
     ]
    }
   ],
   "source": [
    "imgsa = []\n",
    "imgsb = []\n",
    "blurs = []\n",
    "name_type = ['blur', 'threshold']\n",
    "\n",
    "paths = ['/DatasetNN_02/',\n",
    "         '/DatasetNN_06/',\n",
    "         '/DatasetNN_03/',\n",
    "         '/DatasetNN_05/',\n",
    "         '/DatasetNN_07/',\n",
    "         '/DatasetNN_08/',\n",
    "         '/DatasetNN_09/',\n",
    "         '/DatasetNN_10/'\n",
    "]\n",
    "\n",
    "for i in paths:\n",
    "    df = pd.DataFrame.from_csv(f'/raid/rustam/nn_matching{i}res_2.csv', sep=';')\n",
    "    uniq, counts = np.unique(df.index, return_counts=True)\n",
    "    single_values = uniq[counts==1]\n",
    "    \n",
    "    for folder_name in single_values:\n",
    "        \n",
    "        path = os.path.join(f'/raid/rustam/nn_matching{i}', str(folder_name), 'original')\n",
    "        im_1 = os.path.join(path, '1.jpg')\n",
    "        im_2 = os.path.join(path, '2.jpg')\n",
    "        imgsa += [preprocess_input2(cv2.imread(im_1))]\n",
    "        imgsb += [preprocess_input2(cv2.imread(im_2))]\n",
    "        \n",
    "    blurs.append(df[name_type[1]][single_values].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blur3 = df3[name_type[1]][single_values3].values# - df2['blur'][single_values2].values.mean()) / df2['blur'][single_values2].values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl = np.concatenate(blurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl = bl - bl.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model._graph_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [name for name,value in output.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_layers = []\n",
    "for elem in names:\n",
    "    if elem not in layers:\n",
    "        train_layers.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.set_l1_reg([(name,1e-5) for name in train_layers])\n",
    "model.set_l2_reg([(name,4e-4) for name in train_layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rustam/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "one_epoch = int(len(imgsb[:-400]) / batch_size + 1)\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "\n",
    "def get_cosine_decay(lr, steps=100,t_mul=2.0,m_mul=1.0,alpha=1e-4):\n",
    "    global global_step, one_epoch\n",
    "    return tf.train.cosine_decay_restarts(learning_rate=lr,\n",
    "        global_step=global_step,\n",
    "        first_decay_steps=50*steps,\n",
    "        t_mul=t_mul,\n",
    "        m_mul=m_mul,\n",
    "        alpha=alpha,\n",
    "        name='supa_optimizer_lr')\n",
    "\n",
    "def get_piecewise_constant_decay():\n",
    "    global global_step, one_epoch\n",
    "    boundaries = [one_epoch * 40]\n",
    "    boundaries += [boundaries[-1] + one_epoch * 20]\n",
    "    \n",
    "    values = [1e-2, 1e-3, 1e-4]\n",
    "    return tf.train.piecewise_constant(global_step, boundaries,\n",
    "        values, name='supa_optimizer_lr')\n",
    "\n",
    "def get_exp_decay(lr):\n",
    "    global global_step, one_epoch\n",
    "    return tf.train.exponential_decay(lr,\n",
    "       global_step, one_epoch*2, 0.92, staircase=False, name='supa_optimizer_lr')\n",
    "\n",
    "ses.run(tf.initialize_variables([global_step]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = tf.train.AdamOptimizer(learning_rate=1e-4, epsilon=1e-6)\n",
    "opt = tf.train.MomentumOptimizer(learning_rate=get_cosine_decay(1e-2),momentum=0.9,use_nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3304"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgsb[:-400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [00:55<00:00,  1.19it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train cost: 16.9904 Test cost: 17.9109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [00:22<00:00,  2.97it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train cost: 16.6712 Test cost: 17.2452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [00:22<00:00,  2.93it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Train cost: 16.4073 Test cost: 17.2535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [00:22<00:00,  2.96it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Train cost: 16.7906 Test cost: 17.8949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [00:21<00:00,  3.00it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Train cost: 16.3455 Test cost: 17.5694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [00:22<00:00,  2.97it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train cost: 15.9718 Test cost: 16.8884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [00:22<00:00,  2.99it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Train cost: 16.0779 Test cost: 17.1696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [00:22<00:00,  2.98it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Train cost: 15.7619 Test cost: 17.1368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [00:22<00:00,  3.00it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Train cost: 15.5534 Test cost: 17.2947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [00:22<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Train cost: 15.7329 Test cost: 17.0375\n"
     ]
    }
   ],
   "source": [
    "info = model.fit_ce(imgsa[:-400], imgsb[:-400], bl.reshape(-1, 1)[:-400], \n",
    "                    imgsa[-400:], imgsb[-400:], bl.reshape(-1, 1)[-400:],\n",
    "                    opt, epochs=10, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train costs', 'test costs'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXyU9bU/8M+Zmex7ZhIgCySZCWEJEDBgMgGrhRaqVlzqQq+2Vm9rvbba1lZtbW83a69Wr2291l5cav3VUhCw11pErBtLAhI2gSRAEhJIAtnJvk3m/P6YBALOkCGzPDPPc96vFy/MbM8hwifPnPk+50vMDCGEEOqlU7oAIYQQviVBL4QQKidBL4QQKidBL4QQKidBL4QQKmdQugBnTCYTZ2RkKF2GEEIEjT179rQwc5Kz+wIy6DMyMlBaWqp0GUIIETSIqNbVfdK6EUIIlZOgF0IIlZOgF0IIlZOgF0IIlZOgF0IIlZOgF0IIlZOgF0IIlVNN0A/YhvHHj6qw7Viz0qUIIURAUU3Qh+h0eGFrNTburVe6FCGECCiqCXqdjmC1mLCjsgWymYoQQpyjmqAHgCKzEU1dA6hq7la6FDGipXsA24+1KF2GEJqmrqC3mABAgiWAPPveMdz+0i4crOtQuhQhNEtVQZ+eGImpiZHYUdWqdClixNaRH7pPbK5QuBIhtEtVQQ8ARRYjdla3wjZsV7oUzTvZ1ovjLT3ImRSD7ZUtsiJKCIWoLuitZhO6+m041NCpdCmat23kbP6ZW/OQGh+BJzZXwG6XD8qF8DcVBr0RALCjUvr0Stt2rBlT4sIxc0oMHvz8dByq78Q/D55SuiwhNEd1QW+MDsPMKbES9AobtjN2VLZgSbYJRISVeamYMTkGT205gkGbtNWE8CfVBT3gWGZZWtuO/qFhpUvRrE/qzqCz34Yl2Y6dzfQ6wsMrZqC2tRd/231C4eqE0BZ1Br3FhEGbHXtq25UuRbO2HWsB0bklrwBwZU4SLs9MxO/fO4aeAZuC1QmhLaoM+kWZiTDoCNulfaOYbceaMSc1DolRoWdvIyI8/IUZaOkexIvbjitYnRDaosqgjwozYP7UeBRL0Cuiq38Ie0+cwZJs06fuWzA1AStmT8bqrVVo7R5QoDohtEeVQQ84llkerO9AR++Q0qVoTklVK4btjMWWJKf3f395DvqGhvHs+5V+rkwIbVJt0BdZTLAzUFItV8n62/bKFkSG6rFgWrzT+y3J0bh1YTpe21WLk229fq5OCO1RbdDnpccjMlSP4ipp3/jbtmMtKMgyIsygd/mYB5ZOh44IT2854sfKhNAm1QZ9qEGHRZmJsp7ez0bHHjjrz481OS4cdy3OxP8daMDhBhl4JoQvqTboAaDIbEJVcw9Od/QrXYpmjI49GF0/fzHf/IwZseEheHKznNUL4UuqDnqrRcYh+Nvo2ANzUtS4j42LCMF9V5nx0dFmabEJ4UPjBj0RvUxETUR0aMxta4lo/8ivGiLa7+K53yWiw0R0iIjWEFG4N4sfz8zJsUiMCsUOCRG/uHDsgTu+UpiBlLhwPLH5iOwMJoSPuHNG/wqAFWNvYOZbmTmPmfMAbACw8cInEVEqgPsB5DNzLgA9gNs8rvgS6HSEQrMRxZWtEiJ+cOHYA3eEh+jxnc9Nx4GTZ7D50GkfVieEdo0b9My8FUCbs/vIcdp2C4A1Lp5uABBBRAYAkQAaJljnhBWZTTjd2Y/qlh5/H1pznI09cMdNC9IwfVI0fvPOEQzJPgJCeJ2nPfolABqZ+diFdzBzPYCnAJwAcApABzNvcfVCRPQNIiolotLmZu9tUFEkfXq/cTb2wB16HeEHy2eguqUH60pP+qg6IbTL06BfBRdn80SUAGAlgEwAKQCiiOh2Vy/EzKuZOZ+Z85OS3H/rP56piZFIS4iQoPexi409cMeymcnIn5aA3/3rGHoHZeCZEN404aAfacfcCGCti4csA3CcmZuZeQiOPr51osebKCJCkdl09rJ84RvjjT0YDxHhkS/MQFPXAP60o8a7xQmhcZ6c0S8DUMHMdS7uPwGggIgiR3r5SwGUe3C8CbNajOjst+FQvVyY4yvjjT1wR35GIpbNnIQ/fliF9p5BL1YnhLa5s7xyDYASADlEVEdEd4/cdRsuaNsQUQoRbQIAZt4FYD2AvQAOjhxrtRdrd5vV7GgnyDJL33Fn7IE7HlqRg55BG577QAaeCeEt7qy6WcXMU5g5hJnTmPmlkdvvZOY/XvDYBma+eszXP2XmGcycy8x3MLMic2mTYsIwY3IMiitlwJkvuDv2wB3TJ8XgpgVpeLWkFnXtMvBMCG9Q9ZWxY1nNJuyuaZPtBX3gUsYeuOO7n5sOEPDMu59azCWEmADNBH2RxYgBmx17ZXtBr7uUsQfuSImPwJ3WDGzcV4eK051eeU0htEwzQX95lhF6HUmf3ssmMvbAHf9xpRnRYQb8RgaeCeExzQR9dJgBeenx2CF9eq+ayNgDd8RHhuLeK814r6IJHx93emG2EMJNmgl6ACgyG0eCSbYX9JaJjj1wx9esmZgUG4b/ertcZhUJ4QFNBb11ZHvBnVVyVu8tEx174I6IUD2+s2w69p44g3fLGr3++kJohaaCfv7UeESE6FEsQe8Vo2MPFvvgbH7UzZelISspCk++cwQ2GXgmxIRoKujDDHoslO0FvWZ07IG3+/NjGfQ6PLQ8B5VN3diw19VF2EKIi9FU0AOOPv2xpm40dsr2gp7yxtgDdyyfPRl56fF45t1jch2EEBOgvaAfaTPI1nWe89bYg/GMDjw73dmPV4prfHosIdRIc0E/a0os4iNDZJmlh7w59sAdBVlGXJWThD98UImOXlk1JcSl0FzQ63QEq9mI4soWWbLnAW+PPXDHQytmoGvAhj98JAPPhLgUmgt6wDH3pqGjH8dle8EJ8/bYA3fMnBKLG/JS8cqOGpzq6PPbcYUIdpoM+tHlgDtkmeWE+GrsgTu++7npYAZ+KwPPhHCbJoN+mjESqfERKJZllhPiq7EH7khPjMTtBdPw+p6TqGzq8vvxhQhGmgx6IkefvqRathecCF+OPXDHtz5rQWSoAU/KwDMh3KLJoAccIXWmdwhlDTIG91L5cuyBOxKjQnHPFVnYUtaIPTJ2WohxaTborRYjANle8FL5Y+yBO+5ekglTdBieeLtCVk8JMQ7NBn1yTDimT4qWcQiXyB9jD9wRGWrAA8uy8XFNG96vaFK0FiECnWaDHji3veCATS6rd5e/xh6447aF6cgwRuLJzUfksxYhLkLTQV9kMaF/yI69tWeULiVo+GvsgTtC9Dp8f3kOjjR24Y199UqXI0TA0nTQX56VCL2OZO6Nm/w99sAdV+dOwdy0ODzz7lEZeCaEC5oO+tjwEMxNi5M+vZuUGHswHp2O8MiKGag/04e/7KxVuhwhApKmgx4AiswmHKjrQJdsLzguJcYeuMNqMWFJtgn/80GlbBMphBMS9BYThu2MXdWyAfXFKDn2wB0Pr5iBM71D+N+PqpQuRYiAo/mgXzAtHuEhOllPPw4lxx64Izc1DtfNS8FL24+jSTaVEeI8mg/6MIMeCzMSUSzz6S9K6bEH7vj+53MwbGf89j0ZeCbEWJoPesCxnv5IYxeauuRM0BWlxx64Y6oxEl9eNBVrd59EdXO30uUIETAk6HFubHGJjC12KlDGHrjj20uzEW7Q4aktMvBMiFHjBj0RvUxETUR0aMxta4lo/8ivGiLa7+K58US0nogqiKiciAq9Wby3zEqJRVxEiCyzdCFQxh64wxQdhn9fkoVNB09j/0m5EE4IwL0z+lcArBh7AzPfysx5zJwHYAOAjS6e+zsAm5l5BoB5AMo9qNVn9DpCYZYROypbZUCWE4E09sAdX78iC8aoUPzX2+Xy/1MIuBH0zLwVgNO1h+RYZ3cLgDVO7osDcAWAl0ZeZ5CZA/YUq8hiRP2ZPtS29ipdSsAJpLEH7ogOM+D+pdnYWd2Gj442K12OEIrztEe/BEAjMztb5pAJoBnAn4hoHxG9SEQur7Qhom8QUSkRlTY3+/8fZ9HZ7QWlfTNWII49cMeqRVMxNTEST2w+ArsMPBMa52nQr4KTs/kRBgALADzPzPMB9AB4xNULMfNqZs5n5vykJP/3gjNNUZgSFy7LLC9wbuxBcAV9qEGHBz8/HeWnOvHmgQalyxFCURMOeiIyALgRwFoXD6kDUMfMu0a+Xg9H8Ackx/aCJhRXtcgZ4Bjnxh5EK13KJfvi3BTMTonFU1uOyChqoWmenNEvA1DBzHXO7mTm0wBOElHOyE1LAZR5cDyfW5xtRHvvEMpOyfaCQOCPPRiPTkd4eMUM1LX34a+7TihdjhCKcWd55RoAJQByiKiOiO4eues2XNC2IaIUIto05qZvA3iNiD4BkAfgce+U7RtWs6M9IWOLHQJ97IE7lmSbYDUb8ez7lTK4TmiWO6tuVjHzFGYOYeY0Zh5dRXMnM//xgsc2MPPVY77eP9J3n8vM1zNzQO/kPCk2HJbkaOyQPj2A4Bh7MB4ix1l9W88gXth2XOlyhFCEXBl7gSKzER8fb8Ogza50KYoLhrEH7piXHo9lMyfhbx+fkHX1QpMk6C9QZDGhb2gY+04E9JsPnwumsQfuWDozGU1dA6hq7lG6FCH8ToL+ApdnGaEjYIfG594E09gDd1jNRgBAiXz+IjRIgv4CcREhmJMWr/m5N8E29mA8UxMjkRofgZJqbf8AF9okQe9EkdmIAyfPoHvApnQpigm2sQfjISIUmo0oqWqV6ySE5kjQO7HYYoLNzvj4uDbP/oJ17MF4CrMc10lUnO5SuhQh/EqC3okF0xIQZtBpdpllsI49GE/hSJ9erpMQWiNB70R4iB75GQma7dMH89iDi0mJj0CmKQo7pU8vNEaC3gWr2YSK011o6R5QuhS/CvaxB+MpNBuxq7oNtmG5TkJohwS9C6Prx4s1tsxSDWMPLqYwy4iuARsONcg8I6EdEvQu5KbGITbcgGKNtW/UMPbgYgqypE8vtEeC3gW9jlCQZcR2zQW9OsYeuJIUE4acSTGyEbzQFAn6i1icbUJdex9OaGR7QbWNPXCl0GzE7hqZZyS0Q4L+IkbHFmtle0G1jT1wpdBsRP+QHftPBuwWxkJ4lQT9RZiTojApNkwz7Ru1jT1wpSDTCCLp0wvtkKC/CCJCkdmkmcvm1Tb2wJW4yBDkpsRpbkWV0C4J+nEUWUxo6xlU/WXzah174IrVbMT+E2fQNyh7yQr1k6AfR5FFG9sLqnXsgSsFZiMGh+3YU6vtfQeENkjQj2NyXDiykqJU36dX69gDVxZmJMKgI9X/ABcCkKB3S5HZpOrtBdU+9sCZ6DAD5qXHS59eaIIEvRuKLCb0Dg7jQJ06l+OpfeyBK1azEQfrO9DVP6R0KUL4lAS9GwpHthfcfkydb/PVPvbAlcIsI4btjN01bUqXIoRPSdC7IS4yBLmpcart52471ozcFPWOPXBlwbQEhBp0KNbovgNCOyTo3VRkMWHfiTPoUdn2gqNjD7Sy2mas8BA9LpuaIH16oXoS9G4qMo9sL6iyt/laGXvgitVsRPnpTrT3DCpdihA+I0HvpvwMx9v8HSrr02tl7IErhWYjmIFdGt0fWGiDBL2bRt/m71DZ23ytjD1wZW5aPCJD9dK+EaomQX8JFmebUH6qE60q2V5Qa2MPnAk16LAwI1GCXqiaBP0lsJoduxOVqGRzaa2NPXDFajaisqkbTZ39SpcihE9I0F+COalxiAkzYIdKxiFobeyBK6P7DqjlB7gQFxo36InoZSJqIqJDY25bS0T7R37VENH+izxfT0T7iOgtbxWtFINeh8uzjNihgnXXWhx74MqslFjEhhtke0GhWu6c0b8CYMXYG5j5VmbOY+Y8ABsAbLzI8x8AUD7hCgPMYosRJ9p6cbItuLcX1OrYA2f0OsLlWUbp0wvVGjfomXkrAKeLx8lxKngLgDUu7k8DcA2AFz2oMaCMjgkI9vaNVsceuGI1O36A17UH9w9wIZzxtEe/BEAjMx9zcf9vATwEYNyxj0T0DSIqJaLS5uZmD8vyHUtyNJJjwoJ+maVWxx64crZPH+T/X4VwxtOgXwXXZ/PXAmhi5j3uvBAzr2bmfGbOT0oK3HYCEaHIYkJJVQuYg3N7QS2PPXBl+qRoGKNCJeiFKk046InIAOBGAGtdPKQIwHVEVAPgbwA+S0R/mejxAonVbERL9yCONAbn9oJaH3vgDBGhwOzo0wfrD3AhXPHkjH4ZgApmrnN2JzP/kJnTmDkDwG0A3mfm2z04XsAY7WsH69hirY89cMVqNuJ0Zz+Ot/QoXYoQXuXO8so1AEoA5BBRHRHdPXLXbbigbUNEKUS0yftlBpaU+AhkmqKCdpWG1sceuCLr6YVaubPqZhUzT2HmkJEz9JdGbr+Tmf94wWMbmPlqJ6/xITNf672ylVdkMWJXdSuGhoNre8HRsQeLZbXNp2QYIzE5Njxof4AL4YpcGTtBRWYTegaH8UmQbS84OvbgiukS9BciIljNRuysaoXdLn16oR4S9BNUaDaCCNh+LLjO/mTswcUVmo1o7RnE0abg/KBdCGck6CcoPjIUs1NisSOItheUsQfjKxwdXCftG6EiEvQecGwv2I7eweDYXlDGHowvLSESUxMjpU8vVEWC3gNFZhOGhhkfHw+O7QVl7IF7rGYjdlY7rjUQQg0k6D2wMCMRoXpd0Jz9ydgD9xSajejqt+FwQ4fSpQjhFRL0HogI1WP+1PigGHAmYw/cJ316oTYS9B5abDGh7FQn2noGlS7lomTsgfuSY8JhSY4OmndqQoxHgt5DVosJzIF/9idjDy6N1WzE7po2DNqC64I4IZyRoPfQvLQ4RIcZAn6ZpYw9uDRWsxG9QXhBnBDOSNB7yKDXoSArEcUB3KeXsQeX7vJMxwVx0r4RaiBB7wVWswk1rYG7O5GMPbh0CVGhmDk5NuBbckK4Q4LeC0bXpRcH6KbhMvZgYqxmI/acaEf/0LDSpQjhEQl6L5g+KRqm6LCA7NPL2IOJs1qMGLTZsbe2XelShA919g+hOAD/7XqTBL0XOLYXNGJHZeDtTiRjDyZuYUYi9DqSPr2Knerow5eeL8aXX9iFfxxoULocn5Gg95Iiswkt3QM42titdCnnkbEHExcTHoI5qXGyEYlKVTZ14aY/FKPhTD+yk6Pxk/87hKaufqXL8gkJei+xWhxXUwbaVbIy9sAzVrMRB06eQfdAcAyuE+7ZU9uGm54vwZCdsfaeAjx/+2XoGxzGDzccDLh35d4gQe8laQmRmGaMDKhen4w98JzVbILNzthdExyD68T4/lXWiH97cRcSo0Kx8V4rZqfEwZIcjYdWzMB7FU1Yv8fpNthBTYLei4osJuysboMtQLYXlLEHnrtsWgJC9TpZZqkS63afxD1/2YPpk2Kw/puFSE+MPHvf16wZuDwzEb/4Rxnqz/QpWKX3SdB7UZHZhO4BGw7UBcbUQxl74LnRwXUS9MGNmfHcB5V4aMMnKLKYsObrBTBGh533GJ2O8NTN8zDMjIfXf6Kq7SQl6L1odOphoFwlK2MPvKPQbMShhg509A4pXYqYgGE742dvHsZv3jmCG+an4sWv5CMqzOD0semJkfjxNbOwvbIFr+2q9XOlviNB70WJUf7bXpCZ0TtoQ3PXAGpaenC4oQO7a9rw4ZEmbDp4Cq+W1MjYAy+xmh2D63Yel7P6YDNgG8b9a/bhzyW1+PqSTDx98zyEGi4ee6sWpeOK6Ul4fFMFalp6/FSpbzn/sSYmrMhiwis7atA3OIyIUMeZtN3O6BsaRs+gDb0Dw+gesKF38NzXjt9t6BkcRs/ofRc8xvEcx2N6B2zoHRrGeIsDQvSEz85I9sOfWt3y0uMRHuLo0y+fPVnpcoSbOvuHcM+re1BS3YpHr56Jr1+R5dbziAhP3jQXn3/mI3z/9QNYe08h9LrgvthQgt7LrGYjVm+txtKnP8TgsOOsu3fQ/UvoDTpCVJgBUaF6RI7+HmpASnw4IkMNiArTIyrUcO6+MY+JCnP8Hh1mQGSoHvGRIYgJD/Hhn1YbQg06LMxIlD59EGnq7MdX/7Qbxxq78Myt83DD/LRLev7kuHD8YmUuvrN2P17aXo1vXGH2UaX+IUHvZYVmI1YtSkff4PBFQzjqwt9DDYgM00s/PUAVmo14cvMRNHcNICkmbPwnCMVUN3fjKy9/jLaeQbx050J8ZvrEVp2tzEvB5kOn8dQ7R3FlTjKmT4rxcqX+I0HvZWEGPX5941ylyxBeZjWbABzBzupWfHFeitLlCBcOnDyDr72yGwCw5usFmJc+8RVnRITHbsjF8me24nvr9uON/yhCiD44P9YMzqqF8LPclFjEhBlk7k0A++hoM1a9sBNRYXpsuNfqUciPMkWH4Vc35OJQfSee+6DSC1UqQ4JeCDcY9DpcnpWIkgC68lmc88a+Otz9ym5MM0Zhw71WZJqivPbaK3Kn4Ib5qfif9ytxMECukblUEvRCuKkgy4ia1l40qOyqyWD3wtZqfHftASzMSMTaewqQHBPu9WP87IuzYYwOxYOv7w/K/Qkk6IVwk6NPH/gbwWuF3c547K0y/GpTOa6ZMwWv3LUQsT5aZRYXGYInbpqLo43deOZfR31yDF8aN+iJ6GUiaiKiQ2NuW0tE+0d+1RDRfifPSyeiD4iojIgOE9ED3i5eCH+aMTkGCZEh0qcPAIM2O763bj9e3H4cXy2cht+vmu/zFWtX5iRj1aKpWL21Gntqg2vInTtn9K8AWDH2Bma+lZnzmDkPwAYAG508zwbgQWaeBaAAwH1ENMvDeoVQjE5HKDQbUVLVospRtsGiZ8CGu/+8G3/f34AfLM/Bz66b7bcLmh69ZiZS4yPw4LoD6B0MntHV4wY9M28F4PTHFzn2prsFwBonzzvFzHtH/rsLQDmAVI+qFUJhhVlGNHT040RbYG4Er3Yt3QNY9cJOFFe14smb5uK+qyx+3SIzOsyAp26eh5rWXjzxdoXfjuspT3v0SwA0MvOxiz2IiDIAzAew6yKP+QYRlRJRaXNzs4dlCeEbhSN9emnf+N+J1l586fliHG3swuo7LsMtC9MVqaMgy4i7ijLx55LagNtoyBVPg34VnJzNj0VE0XC0d77DzJ2uHsfMq5k5n5nzk5JkfroITOakKCTHhEnQ+9mh+g7c+Hwx2nuH8Nq/F2DpzEmK1vPQihxkJUXhofWfoLM/8KeaTjjoicgA4EYAay/ymBA4Qv41ZnbWxxciqBARrGYjSqoCbyN4tSqubMFtq3ciVE/YcG8hLpuWoHRJCA/R4+mb5+FURx8ee6tM6XLG5ckZ/TIAFczsdN+tkf79SwDKmfm/PTiOEAGl0GxES/cAKpsCayN4NXrrkwbc+afdSIkPx4b/sMKSHDjzZuZPTcC9V5qxrrQO75U3Kl3ORbmzvHINgBIAOURUR0R3j9x1Gy5o2xBRChFtGvmyCMAdAD47Zinm1V6sXQhFWFXQp2/s7A/4lsMrO47j22v2YV56HF6/x4opcRFKl/Qp9y/NxozJMXhk40G09wwqXY5LFIhvP/Pz87m0tFTpMoRwafET72N2Siz+9458pUu5ZEcbu3Dt77djcNiO9MQIzJoSi1lT4jArJRazUmKREhfu15UsF2JmPLXlCJ77oAqfmzUJz66aj/CQwJ3qWtbQiZXPbceK3Cl4dtV8xeogoj3M7PQvpEyvFGICrGYj3jncCLudoQuiTSnsdsaPNh5EVJge9y+2oPx0F8obOrGlrPHsRjZxESGO8E+JxcwpsZg1JRaW5Ohxd2byBtuwHT/ceBCv76nDqkVT8cuVs2EI8ImRs1Ji8cDSbDy15SiWz56Ea+cG3nRTCXohJqDQbMS60jqUnepEbmqc0uW4bV3pSZTWtuPJL83FLfnnlif2DNhQcboLZac6UdbQibJTnfjLzloM2OwAHLuVZSfHOM76x/wQiIvw3siBvsFhfOuve/FeRRPuX5qN7y7LVvSdxaX45mfMeLesET/5+yEsykz0ybwdT0jQCzEBhVnn5t4ES9C3dA/g129XYFFmIm6+7Pwdl6LCDLhsWsJ5K1psw3bUtPag7FTX2fD/8EgT1u85t/4iLSHiU2f/aQkRlxzQ7T2DuPvPu7Hv5Bk8dn0ubi+Y5tkf1s8Meh2eviUP1/x+G3644SBe/Gp+QP2QkqAXYgImx4UjKykKxVUtbu9FqrTH/1mO3kEbHr8h160QMuh1sCTHwJIcg+vGbLbS1NV/NvhHf3+3/FzrJzbc4Aj9MWf/2ckxLls/9Wf68JWXduFkex+e/7cFWJE7xSt/Xn+zJEfjB8tz8Ng/y7F+Tx1uzlfmgi5nJOiFmCCr2Yg39tZjaNge8DsP7ahswcZ99fj2Zy0eL1FMjglHck44rsw5t/F876Cj9VM+Jvz/9vFJ9I2M9A3REyzJMWeDf9bI2f/pzn585eVd6B0cxqt3LUJBltGj2pR2V1EmtpQ14hf/KIPVYkJqfGCsFJJVN0JM0D8/OYX7/roXG+61BsRFPK70Dw3jC7/bBjsz3vnOFX5bwTJsZ0fr54Kz/+augbOPMegIiVGhePXuRZgxOdYvdfnaidZerPjdViyYmoBX71rktw/rZdWNED5QkJUIANhZ3RrQQf/8h1U43tKD/3f3Ir8uU9TrCOakaJiTos/bZ7e5a8Bx5n+qE42d/birKBPpiZF+q8vXphoj8eg1M/HoG4fw2q5a3FGYoXRJEvRCTJQxOgwzJseguKoF911lUbocp6qau/H8h1VYmZeCJdmBMUMqKSYMSTFJuGJ6YNTjC19eNBXvHG7E45sqsCQ7CRle3NpwIgK7sShEgLOaTSitaQ/I7eWYGY++cRDhITr8+BrZCsKfiAhP3jQXIXrC918/gGG7si1yCXohPFBoNmLAZse+E2eULuVTNu6tx87qNjz8hRlIiglTuhzNmRwXjp+vnI3S2na8tL1a0Vok6IXwwKLMROgIKKkOrLk37T2D+NWmciyYGo9VC6cqXY5mXZ+XiuWzJ+Gpd47iaGOXYnVI0AvhgbiIEMxJjUNJVWBtQPHrt8vR2TeEx2+cE1QjGtSGiFwaX8YAAArESURBVPCrG+YgOtyAB9cdwNCwXZE6JOiF8FCh2YR9J84EzB6iHx9vw7rSOty9JFM1SxaDmSk6DI/fkIuD9R34wwdVitQgQS+Eh6xmI2x2xu6adqVLwaDNjh+9cRBpCRF4YGm20uWIEStyp+D6vBQ8+/4xHKrv8PvxJeiF8FB+RgJC9ISSAJhP/8K2alQ2deOXK3MRGSqrpwPJz6/LhTE6FN9btx8DNv+u0pKgF8JDkaEG5KXHK96nr23twe/fO4ar50zGVTOSx3+C8Ku4yBD8101zcbSxG//97lG/HluCXggvKDSbcLC+Ax19yuzaxMz48d8PIUSvw0+/OFuRGsT4rspJxqpF6Vi9tRp7atv8dlwJeiG8wGo2ws6OD0KV8I9PTmHbsRb8YHkOJsUG1ix0cb5Hr5mF1PgIPLjugN8+wJegF8IL5k+NR5hBp0ifvqNvCL/4RxnmpsUF3Rx3LYoOM+A3X5qHmtZePPF2hV+OKUEvhBeEGfTIz0hAsQJ9+ic3V6CtZwCP3zAHelkzHxQKzUZ8rSgDfy6pxY5K3/+dkaAXwkusZhMqTnehtXtg/Ad7yd4T7fjrxyfwtaLMoNnpSjg8tHwGskxReGj9J+js9+1nOxL0QnhJodmxacbOav/06YeG7fjRxoOYHBuO731uul+OKbwnIlSPp26Zh1MdfXjsrTKfHkuCXggvmZMah6hQvd/aNy9vP46K01342XWzERUma+aD0YKpCfjmZ8xYV1qH98obfXYcCXohvCREr8OizES/DDira+/Fb/91DMtmTsLy2ZN9fjzhOw8sy8aMyTF4ZONBtPcM+uQYEvRCeJHVbEJ1cw9Od/T77BjMjJ/+32EQAT9fKWvmg12YQY+nb5mH9p5B/Oebh31yDAl6IbxotE9fUu279s07h0/jvYomfO9z0wNm82nhmdkpcXhgaTZOnenzydp6CXohvGjWlFjERYSguNI37Zuu/iH89M3DmDUlFndaM3xyDKGMe680Y+09hT6ZUSSf4AjhRTodoSDLd336p7ccRVPXAP73jnwY9HKepia+/P8pf1OE8DKr2YS69j6cbOv16userOvAqyU1uKNgGvLS47362kLdxg16InqZiJqI6NCY29YS0f6RXzVEtN/Fc1cQ0REiqiSiR7xZuBCByjrSp/fmMkvbsB0/fOMTmKLD8P3lOV57XaEN7pzRvwJgxdgbmPlWZs5j5jwAGwBsvPBJRKQH8ByALwCYBWAVEclW9EL1LMnRMEWHodiLc29eLanFofpO/OcXZyE2PMRrryu0YdygZ+atAJxe6kdEBOAWAGuc3L0IQCUzVzPzIIC/AVjpQa1CBAUiQqHZiJKqVjCzx693qqMPT285gitzknDNnCleqFBojac9+iUAGpn5mJP7UgGcHPN13chtThHRN4iolIhKm5ubPSxLCGVZzUY0dQ2gqrnH49f6+ZtlGGbGL1fmwnFuJcSl8TToV8H52fwlY+bVzJzPzPlJSUneeEkhFDPap/d016l/lTVi8+HTuH9pNtITI71RmtCgCQc9ERkA3AhgrYuH1ANIH/N12shtQqje1MRIpMSFe9Sn7x204advHsb0SdH4+pIsL1YntMaTM/plACqYuc7F/bsBZBNRJhGFArgNwJseHE+IoOHo05uws7oVdvvE+vS//dcx1J/pw+M3zEGIrJkXHnBneeUaACUAcoiojojuHrnrNlzQtiGiFCLaBADMbAPwLQDvACgHsI6ZfTPIQYgAZDUb0d47hIrTXZf83LKGTry0/ThWLUpHfkaiD6oTWjLulbHMvMrF7Xc6ua0BwNVjvt4EYJMH9QkRtArHrKeflRLr9vOG7YwfvXEQ8REheHjFDF+VJzRE3g8K4SMp8RHIMEZe8j6yf/34BPafPIOfXDsL8ZGhPqpOaIkEvRA+VGg2YdfxNtiG7W49vqmrH09ursBiiwkr81J8XJ3QCgl6IXzIajaie8CGQw2dbj3+l2+VY8Bmxy+vlzXzwnsk6IXwoYIs9+fefHS0Gf840ID7rrQg0xTl69KEhkjQC+FDSTFhmD4petw+ff/QMH7y90PISorCN6+UNfPCuyTohfAxq9mE3TVtGLANu3zMs+8fw4m2Xvzq+jkIM+j9WJ3QAgl6IXys0GxE/5AdB052OL3/aGMXVm+txk0L0s4uyRTCmyTohfCxgkwjiJz36e12xqNvHERUmAGPXjNTgeqEFkjQC+FjcZEhyE2Jczr35vU9J7G7ph0/unomEqNkzbzwDQl6Ifyg0GzEvhPt6Bs816dv7R7Ar9+uwKLMRNx8WZqC1Qm1k6AXwg8KzUYMDTP21Lafve1Xm8rRM2DD4zfImnnhWxL0QvjBwoxEGHR0tk9fXNmCjXvr8c3PmGFJjlG4OqF2EvRC+EF0mAHz0uNRXNWKAdswfvz3Q5hmjMR9V1mULk1ogAS9EH5SmGXEwfoO/GbzEVS39OCx63MRHiJr5oXvSdAL4SdWsxHDdsaL24/junkpWJItW2YK/xh3Hr0QwjsWTEtAqEGHcIMOP75W1swL/5GgF8JPwkP0+Mk1M5GWEInkmHClyxEaIkEvhB/dUZihdAlCg6RHL4QQKidBL4QQKidBL4QQKidBL4QQKidBL4QQKidBL4QQKidBL4QQKidBL4QQKkfMrHQNn0JEzQBqJ/h0E4BP79mmTfK9OJ98P84n349z1PC9mMbMTgcoBWTQe4KISpk5X+k6AoF8L84n34/zyffjHLV/L6R1I4QQKidBL4QQKqfGoF+tdAEBRL4X55Pvx/nk+3GOqr8XquvRCyGEOJ8az+iFEEKMIUEvhBAqp5qgJ6IVRHSEiCqJ6BGl61ESEaUT0QdEVEZEh4noAaVrUhoR6YloHxG9pXQtSiOieCJaT0QVRFRORIVK16QkIvruyL+TQ0S0hohUt/2XKoKeiPQAngPwBQCzAKwiolnKVqUoG4AHmXkWgAIA92n8+wEADwAoV7qIAPE7AJuZeQaAedDw94WIUgHcDyCfmXMB6AHcpmxV3qeKoAewCEAlM1cz8yCAvwFYqXBNimHmU8y8d+S/u+D4h5yqbFXKIaI0ANcAeFHpWpRGRHEArgDwEgAw8yAzn1G2KsUZAEQQkQFAJIAGhevxOrUEfSqAk2O+roOGg20sIsoAMB/ALmUrUdRvATwEwK50IQEgE0AzgD+NtLJeJKIopYtSCjPXA3gKwAkApwB0MPMWZavyPrUEvXCCiKIBbADwHWbuVLoeJRDRtQCamHmP0rUECAOABQCeZ+b5AHoAaPYzLSJKgOPdfyaAFABRRHS7slV5n1qCvh5A+piv00Zu0ywiCoEj5F9j5o1K16OgIgDXEVENHC29zxLRX5QtSVF1AOqYefQd3no4gl+rlgE4zszNzDwEYCMAq8I1eZ1agn43gGwiyiSiUDg+THlT4ZoUQ0QERw+2nJn/W+l6lMTMP2TmNGbOgOPvxfvMrLozNncx82kAJ4koZ+SmpQDKFCxJaScAFBBR5Mi/m6VQ4YfTBqUL8AZmthHRtwC8A8en5i8z82GFy1JSEYA7ABwkov0jt/2ImTcpWJMIHN8G8NrISVE1gK8pXI9imHkXEa0HsBeO1Wr7oMJxCDICQQghVE4trRshhBAuSNALIYTKSdALIYTKSdALIYTKSdALIYTKSdALIYTKSdALIYTK/X8tNMNPbgnPzAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(info['test costs'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-dfa009d91b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(2==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = model.predict([imgsa[-batch_size:], imgsb[-batch_size:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = answer - bl.reshape(-1, 1)[-batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
